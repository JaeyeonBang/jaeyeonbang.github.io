---
title: "Seq2Seq"
date: 2025-11-20 12:02:00 +0900
categories: [Neural Network, Base Paper study]
tags: [NLP, LSTM, RNN]
media_subpath: /assets/img/Seq2Seq/
use_math: true
---
[[paper link]](https://arxiv.org/abs/1409.3215)

자연어 처리에 LSTM을 활용한 Encoder-decoder 모델을 제시한 Seq2Seq 논문을 간략하게 정리하려고 합니다.

실험이나 결과관련 내용보다, 본 논문이 제시한 개념적 특성을 정리하였습니다.

__*아래 내용은 간략하게 논문을 정리한 내용입니다. 정리, 번역, 표현에는 오류가 있을 수 있습니다.*__

<br><br>
# Intro

- DNN은 강력한 도구로 활용될 잠재력을 보여줍니다.
- 그러나, DNN은 fixed 된 dimension의 vector만 처리할 수 있다는 단점을 지니고 있습니다. (e.g. 길이가 얼마나 될 지 모르는 동영상, 문서 등은 효과적으로 처리할 수 없다)
- 즉 input과 output의 dimensionality가 고정되지 않았을 때 처리하기 위한 방안이 필요합니다.
- 본 연구를 이를 위해 LSTM 아키텍처를 활용합니다.
- ![image](20251121_121954_image.png)
- 하나의 LSTM이 한번에 하나씩 input 정보를 읽고, 다른 LSTM이 output을 산출합니다.
- LSTM은 다른 RNN에 비해 sequence의 길이에 의해 영향을 덜 받는 경향이 있습니다.

<br><br>

# The model


- Standard RNN에서 input이 $(x_1, ... , x_T)$로 주어질 때, output $ (y_1, ... y_T)$는 다음과 같은 과정을 반복하며 산출됩니다.

$$
h_t = sigm(W^{hx}{x_t} + W^{hh}h_{t-1})\\
y_t = W^{yh}h_t
$$

- input과 weight를 행렬곱한 값과 hidden과 weight를 행렬곱한 값을 통해 새로운 hidden이 갱신되며,
- hidden과 weight를 행렬곱하여 output이 나오게 됩니다.


- LSTM의 목표는 입력 시퀀스 $(x_1, \dots, x_T)$가 주어졌을 때, 이에 대응하는 출력 시퀀스 $(y_1, \dots, y_{T})$가 등장할 조건부 확률 
$p(y,\dots,y_T | x, \dots, x_T)$를 추정하는 것입니다.  


<br>

- **1단계: 인코딩 (압축)**
  * 입력 시퀀스 $(x_1, \dots, x_T)$를 LSTM에 통과시키고, 이를 통해 입력 전체 정보를 요약한 표현 벡터(fixed-dimensional representation)$v$를 산출합니다.  
  <br>
- **2단계: 디코딩 (생성)**
  * 출력 시퀀스 $(y_1, \dots, y_{T'})$의 확률을 계산하기 위해 또 다른 표준 LSTM 언어 모델(Standard LSTM-LM)을 사용합니다.
  * **가장 중요한 연결 고리:** 이 두 번째 LSTM(디코더)의 **초기 은닉 상태(initial hidden state)**를 앞서 구한 입력의 표현 벡터 **$v$**로 설정합니다.

$$
p(y_1, \dots, y_{T'} | x_1, \dots, x_T) = \prod_{t=1}^{T'} p(y_t | v, y_1, \dots, y_{t-1})

$$

- 각 시점의 확률 예측은 모든 단어들에 대해 Softmax를 취하여 계산됩니다.
- 문장 끝에는 <EOS\> 토큰을 사용하여 문장의 끝임을 알수 있게 합니다.
- layer의 크기가 클 수록 성능이 향상됨을 확인했으며, 4 layer를 지는 LSTM을 사용합니다.
- 문장을 거꾸로 input으로 넣었을 때 성능이 향상됨을 확인하였습니다.(e.g. c , b, a)
