---
title: "RLPR: EXTRAPOLATING RLVR TO GENERAL DO MAINS WITHOUT VERIFIERS"
date: 2026-01-03 14:00:00 +0900
categories: [Neural Network, RL]
tags: [RL, NLP, LLM]
media_subpath: /assets/img/RLPR/
use_math: true
---
[[paper link]](https://arxiv.org/pdf/2506.18254)

### í•œ ì¤„ ì„¤ëª…

**ì •ë‹µì¸ì§€ ì•„ë‹Œì§€ ê·œì¹™ìœ¼ë¡œ íŒì •í•˜ì§€ ì•Šê³  ëª¨ë¸ì´ reference answerë¥¼ ì–¼ë§ˆë‚˜ ë†’ì€ í™•ë¥ ë¡œ ì˜ˆì¸¡í•˜ëŠ”ì§€ë¥¼ ëª¨ë¸ì˜ í™•ì‹ ìœ¼ë¡œ ë³´ê³  ê°•í™”í•™ìŠµ rewardë¡œ ì‚¬ìš©í•œë‹¤.** 

[[Git]](https://github.com/OpenBMB/RLPR)

### ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„

- Verifiable Rewardsë¥¼ í™œìš©í•œ Reinforcement Learning(RLVR)ì€ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° í° ì—­í• ì„ í•˜ê³  ìˆìŒ
    
    ![image.png](image.png)
    
    â†’ BUT ì´ê±´ ì—¬ì „íˆ ìˆ˜í•™, ì½”ë“œ ë„ë©”ì¸ì— ì œí•œë˜ì–´ìˆìŒ
    - ê¸°ì¡´ RLVR ë°©ë²•ë“¤ì´ rewardë¥¼ ì–»ê¸° ìœ„í•´ domain-specific verifierì— í¬ê²Œ ì˜ì¡´í•˜ê¸° ë•Œë¬¸
    - ë•Œë¬¸ì— ì´ëŸ¬í•œ rule-based reward systemì„ ìƒˆë¡œìš´ ëª¨ë¸ì´ë‚˜ ë„ë©”ì¸ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë°ì—ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê³¼ë„í•œ heuristic engineeringì´ ìš”êµ¬ë¨
    - free-form answerë¥¼ ê°–ëŠ” ì¼ë°˜ ë„ë©”ì¸ ì¶”ë¡ ì˜ ê²½ìš° ìì—°ì–´ì˜ ë†’ì€ ë‹¤ì–‘ì„±ê³¼ ë³µì¡ì„± ë•Œë¬¸ì— rule-based verifierë¥¼ ì„¤ê³„í•˜ëŠ” ê²ƒ ìì²´ê°€ ì‚¬ì‹¤ìƒ ë¶ˆê°€ëŠ¥
    
    
    
    - ì¼ë°˜ì ì¸ reward evaluationì„ ìœ„í•´ LLMì„ í•™ìŠµì‹œí‚¤ë©´ ë˜ì§€ ì•Šë‚˜?
    
    - ê·¸ëŸ¬ë‚˜ ì¼ë°˜ì ì¸ reward evaluationì€ ì‰½ì§€ ì•Šê³  ë°©ëŒ€í•œ ë°ì´í„° annotationì„ í•„ìš”ë¡œ í•¨
    - ì‹¤ì œë¡œëŠ” ë§Œì¡±ìŠ¤ëŸ½ì§€ ëª»í•œ reward qualityë¡œ ì´ì–´ì§€ëŠ” ê²½ìš°ê°€ ë§ìŒìŒ
    

### ì—°êµ¬ ëª©í‘œ

- RLì„ ë³´ë‹¤ ë„“ì€ ì¼ë°˜ ë„ë©”ì¸ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆëŠ” Reward framworkë¥¼ ë§Œë“œëŠ” ê²ƒ
- ì •ë‹µì¸ì§€ ì•„ë‹Œì§€ ê·œì¹™ìœ¼ë¡œ íŒì •í•˜ì§€ ì•Šê³  ëª¨ë¸ì´ reference answerë¥¼ ì–¼ë§ˆë‚˜ ë†’ì€ í™•ë¥ ë¡œ ì˜ˆì¸¡í•˜ëŠ”ì§€ë¥¼ ëª¨ë¸ì˜ í™•ì‹ ìœ¼ë¡œ ë³´ê³  ê°•í™”í•™ìŠµ rewardë¡œ ì‚¬ìš©

### ë°©ë²•ë¡ 

- **Reinforcement Learning from Verifiable Reward(RLVR)**
    - rule-based verifierê°€ ê° ìƒì„±ëœ ì‘ë‹µì— ëŒ€í•´ scalar reward scoreë¥¼ ë¶€ì—¬í•˜ëŠ” ì¼ë°˜ì ì¸ post-training íŒ¨ëŸ¬ë‹¤ì„
    - í”„ë¡¬í”„íŠ¸ xê°€ ì£¼ì–´ì§€ë©´ policy $\pi_Î¸â€‹$ ëŠ” reasoning content zì™€ ìµœì¢… ë‹µë³€ yë¥¼ êµ¬ì„±
        
        â†’ ì´í›„ ê¸°ëŒ€ verifier score $J(Î¸â€‹)$ë¥¼ ìµœì í™”í•¨
        
        ![image.png](image%201.png)
        
        â†’ ì¦‰ verifierê°€ YESë¼ê³  ë§í•  í™•ë¥ ì„ ë†’ì´ë„ë¡ ëª¨ë¸ íŒŒë¼ë¯¸í„° ğœƒë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤ëŠ” ì˜ë¯¸
        
        - $f_{verifierâ€‹}$: ìƒì„±ëœ ë‹µë³€ $y$ê°€ GT $y*$ë¡œ ì •ì˜ëœ testë¥¼ í†µê³¼í•˜ëŠ”ì§€ ê²€ì‚¬í•˜ëŠ” task-specific, rule-based verifier
            
            â†’ RLVRì˜ í° í•œê³„: ìˆ˜í•™, ì½”ë“œì²˜ëŸ¼ ëª…í™•í•œ ê·œì¹™ì´ ìˆëŠ” ë¶„ì•¼ëŠ” ê´œì°®ì§€ë§Œ ê¸€ì“°ê¸°, ìš”ì•½, ì°½ì˜ì  ì¶”ë¡  ê°™ì€ ë¶„ì•¼ë¡œëŠ” í™•ì¥í•˜ê¸° ì–´ë µë‹¤.
            
**RLPR**

- **ì™¸ë¶€ verifier ì—†ì´ ì¼ë°˜ ë„ë©”ì¸ RLVRì„ í™•ì¥í•˜ëŠ” RLPR(Reinforcement Learning with Reference Probability Reward) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆ**
- reference answerì— ëŒ€í•œ LLM ìì²´ì˜ token probability scoreë¥¼ reward signalë¡œ ì‚¬ìš©
    
    - ì¦‰, LLMì´ ìƒì„±í•  intrinsic probability ìì²´ê°€, í•´ë‹¹ ì¶”ë¡  ê³¼ì •ì´ ì–¼ë§ˆë‚˜ ì˜ ì •ë‹µì— ë„ë‹¬í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” reasoning rewardë¡œ í™œìš©, ìƒì„±ëœ ë‹µë³€ì— ëŒ€í•œ ëª¨ë¸ ìì‹ ì˜ í‰ê°€ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë°˜ì˜í•œë‹¤ëŠ” ì 

![image.png](image%202.png)

**ë³´ìƒ**

- ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ $o = (o_0, ..., o_N)$ì´ë¼ê³  í•  ë•Œ (oëŠ” ê° í† í° ë‹¨ìœ„)
    - ëª¨ë¸ì´ ìƒì„±í•œ ì¶”ë¡  $z$ì™€ ë‹µë³€ $y$ ì¤‘ ë‹µë³€ $y$ë¥¼ ë²„ë¦¬ê³  GT $y*$ë¥¼ ëŒ€ì‹  ë„£ëŠ”ë‹¤.
        
        â†’ ì´ reasoningì„ í–ˆì„ ë•Œ, ëª¨ë¸ì€ ì§„ì§œ ì •ë‹µì„ ì–¼ë§ˆë‚˜ ê·¸ëŸ´ë“¯í•˜ê²Œ ìƒê°í•˜ëŠ”ê°€?
        
    - reasoningì„ ì£¼ê³  ê·¸ ë‹¤ìŒì— ì •ë‹µì´ ë‚˜ì˜¬ í™•ë¥ ì€ ì–´ë–»ê²Œ ë˜ëŠ”ì§€
        
        ```markdown
        y* = "New York City"
        
        pâ‚ = P("New"  | ì• ë¬¸ë§¥)
        pâ‚‚ = P("York" | ì• ë¬¸ë§¥ + "New")
        pâ‚ƒ = P("City"| ì• ë¬¸ë§¥ + "New York")
        ```
        
        - ì´ë¥¼ í•˜ë‚˜ì˜ ì ìˆ˜ë¡œ â‡’ Probability reward
            
            ![image.png](image%203.png)
            
            = ì •ë‹µ í† í° í‰ê·  í™•ë¥ 
            
            - reasoning zë¥¼ ì˜ ì“°ë©´, ì´í›„ í† í° probabilityê°€ ì¦ê°€, reward ì¦ê°€
            - reasoning zë¥¼ ëª» ì“°ë©´, ì´í›„ í† í° probabilityê°€ ì¦ê°€, â†’ reward ê°ì†Œì†Œ
        - $f_{seq}$: per-token probabilityë¥¼ í•˜ë‚˜ì˜ scalar rewardë¡œ ëª¨ìœ¼ëŠ” í•¨ìˆ˜
        
            - mean probabilityì„ í™œìš©í•¨í•¨
                
                ![image.png](image%204.png)
                
                - ë” robustí•œ reward signalì„ ì œê³µ
                
                â†’ ì±„íƒ
                

**ì¶”ë¡ ì˜ ìˆœìˆ˜í•œ ê¸°ì—¬ë§Œ ë³´ìƒí•´ì•¼ í•œë‹¤.** 

- ë¬¸ì œê°€ ì‰¬ì› ì„ ìˆ˜ë„ ìˆê³ , ì •ë‹µì´ í”í•œ í‘œí˜„ì¼ ìˆ˜ ìˆê³ â€¦ ë“±ë“± ì¶”ë¡ ê³¼ ë¬´ê´€í•œ ìš”ì¸ë“¤ì´ ë³´ìƒì— ì„ì—¬ìˆìŒ
- SO, ì¶”ë¡ ì´ ì—†ì´ ì •ë‹µë§Œ decodingí–ˆì„ ë•Œ ê³„ì‚°í•œ probability rewardë¥¼ baseline rewardë¡œ ë‘ê³ 
    
    ![image.png](image%205.png)
    
    ![image.png](image%206.png)
    
    - $r$: ì¶”ë¡  í¬í•¨ ìƒíƒœì—ì„œì˜ reward
    - $r^â€²$: ì¶”ë¡  ì—†ì´ë„ ì–»ëŠ” ê¸°ë³¸ reward

**ìµœì¢… objective functionì— ëŒ€í•œ gradient estimator**

![image.png](image%207.png)

- debiased reward $\hat r$ê°€ í° ì‘ë‹µ $o$ê°€ ë” ìì£¼ ë‚˜ì˜¤ë„ë¡ $\pi_Î¸â€‹$ë¥¼ ì—…ë°ì´íŠ¸

**ë„ˆë¬´ ì‰½ê±°ë‚˜ ì–´ë ¤ìš´ ë¬¸ì œë“¤ì€ í•„í„°ë§í•œë‹¤.** 

- í•˜ì§€ë§Œ rewardê°€ 0~1 ì‚¬ì´ ì—°ì†ê°’ì´ê¸° ë•Œë¬¸ì— thresholdë¥¼ ì •í•˜ê¸° ì–´ë µê³ â€¦
    
    â†’ í•œ promptì— ëŒ€í•´ rewardë“¤ì˜ ë¶„ì‚°(standard deviation)ì„ ë³´ë©´ ê·¸ ë¬¸ì œê°€ ì‰¬ìš´ì§€/ì–´ë ¤ìš´ì§€ ì•Œ ìˆ˜ ìˆë‹¤!
    
    - rewardì˜ í‘œì¤€í¸ì°¨ê°€ ë‚®ì€ ê²½ìš° -> ë¬¸ì œê°€ ë„ˆë¬´ ì‰½ê±°ë‚˜ ì–´ë ¤ì›€
- rewardì˜ standard deviationì´ ì‘ì€ promptëŠ” ë²„ë¦°ë‹¤.
    - í•˜ì§€ë§Œ í•™ìŠµ ì´ˆë°˜, ì¤‘ë°˜, í›„ë°˜ì— ë”°ë¼ reward ë¶„í¬ê°€ ê³„ì† ë³€í•˜ê¸° ë•Œë¬¸ì— ë™ì  threshold + EMA ì ìš©
        

### ì‹¤í—˜ ê²°ê³¼
![image.png](image%208.png)


## ì½”ë“œ ë¶„ì„ ìš”ì•½
- í•´ë‹¹ git ì—ì„œëŠ” reward manager ì•ˆ prob.pyì—ì„œ ê´€ë¦¬
- score aì™€ ê°™ì´ forwardë¥¼ í†µí•œ í™•ë¥ ê³„ì‚°ì€ trainer ë‚´ë¶€ì˜ rolloutì„ í†µí•´ ê³„ì‚°ë¨

- Score Bë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •
```python
def compute_scoreB(self, old_log_probs, ground_truth_mask):
        """
        ëª¨ë¸ì´ ìƒì„±í•œ ì •ë‹µ í† í°ë“¤ì˜ ë¡œê·¸ í™•ë¥ ì„ ë°”íƒ•ìœ¼ë¡œ scoreB(r) ê°’ì„ ê³„ì‚°í•¨.
        ë¡œê·¸ í™•ë¥ ì„ ì‹¤ì œ í™•ë¥ ë¡œ ë³€í™˜í•˜ê±°ë‚˜ í‰ê· ì„ ë‚´ëŠ” ë“± ë‹¤ì–‘í•œ ìˆ˜ì¹˜ì  ë³€í™˜ì„ ìˆ˜í–‰.
        Score B -> zê°€ ìˆì„ ë•Œ ground truthê°€ ë‚˜ì˜¬ í™•ë¥  ê´€ë ¨ ì ìˆ˜ (ë…¼ë¬¸ì—ì„œ r)
        """
        # ì •ë‹µ í† í°ì´ ë§ˆìŠ¤í‚¹ ìƒì— ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì ìˆ˜ëŠ” 0
        if ground_truth_mask.sum() == 0:
            scoreB = 0
        else:
            # 1. ì •ë‹µ(Ground Truth) ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë¡œê·¸ í™•ë¥ ê°’ë“¤ë§Œ í•„í„°ë§í•˜ì—¬ ì¶”ì¶œ
            # 
            old_log_probs_in_gt = old_log_probs[ground_truth_mask.bool()]
            
            # 2. ë§ˆì§€ë§‰ í† í° í™•ë¥  ì¡°ì •(Clipping) ë¡œì§
            # ë§ˆì§€ë§‰ ì •ë‹µ í† í°ì˜ í™•ë¥ ì´ ë„ˆë¬´ ë†’ì„ ë•Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë³´ìƒ í­ì£¼ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ìµœëŒ€ì¹˜ë¥¼ ln(0.5)ë¡œ ì œí•œ
            if self.gt_tokens_one_more and self.gt_tokens_one_more_adjusted:
                old_log_probs_in_gt[-1] = min(old_log_probs_in_gt[-1], np.log(0.5))
            
            # 3. ê³ ì •ë°€ë„ ê³„ì‚°ì„ ìœ„í•´ float32(BF32 ìˆ˜ì¤€)ë¡œ ë³€í™˜
            old_log_probs_in_gt = old_log_probs_in_gt.to(torch.float32)
            
            # 4. ì„¤ì •ëœ ê³„ì‚° ë°©ì‹(compute_score_name)ì— ë”°ë¥¸ ìˆ˜ì¹˜ ì‚°ì¶œ
            # ë°©ì‹ A: ê° í† í° í™•ë¥ (exp)ì˜ ì‚°ìˆ  í‰ê·  (ê°€ì¥ ì¼ë°˜ì ì¸ RLPR ë°©ì‹)
            if self.compute_score_name == 'mean_exp_log_softmax':
                scoreB = torch.mean(torch.exp(old_log_probs_in_gt)).item()
            
            # ë°©ì‹ B: ë¡œê·¸ í™•ë¥ ê°’ ìì²´ì˜ ì‚°ìˆ  í‰ê· 
            elif self.compute_score_name == 'mean_log_softmax':
                scoreB = torch.mean(old_log_probs_in_gt).item()
            
            # ë°©ì‹ C: ëª¨ë“  í† í° í™•ë¥ ì˜ ê³± (ì „ì²´ ì‹œí€€ìŠ¤ê°€ í•œ ë²ˆì— ë‚˜ì˜¬ í™•ë¥ )
            elif self.compute_score_name == 'exp_sum_log_softmax':
                scoreB = torch.exp(torch.sum(old_log_probs_in_gt)).item()
            
            # ë°©ì‹ D: ê¸°í•˜ í‰ê·  (í™•ë¥  ê³±ì˜ nì œê³±ê·¼ê³¼ ìœ ì‚¬í•œ íš¨ê³¼)
            elif self.compute_score_name == 'exp_mean_log_softmax':
                scoreB = torch.exp(torch.mean(old_log_probs_in_gt)).item() 
            else:
                raise ValueError
            
            # 5. ìµœì¢… ê²°ê³¼ë¥¼ í•™ìŠµ í™˜ê²½ì— ë§ëŠ” ë°ì´í„° íƒ€ì…(BF16)ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ì—¬ ì €ì¥
            scoreB = torch.tensor(scoreB, dtype=torch.bfloat16).item()

        return scoreB

    def compute_scoreA_scoreB_and_extracted_answer(self, data_item, valid_response_ids):
        """
        ë°ì´í„° ì†ŒìŠ¤ì™€ ì„¤ì •ì— ë”°ë¼ VR(ì‹¤ì œ ì±„ì ) ë˜ëŠ” PR(í™•ë¥  ê¸°ë°˜ ì±„ì )ì„ ë¶„ê¸°í•˜ì—¬ ìˆ˜í–‰.
        ê²°ê³¼ì ìœ¼ë¡œ scoreA(r'), scoreB(r), ê·¸ë¦¬ê³  ì¶”ì¶œëœ ì •ë‹µ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•¨.
        """
        data_source = data_item.non_tensor_batch['data_source']
        
        # --- ë¶„ê¸° 1: ì‹¤ì œ ì •ë‹µ ê²€ì¦(VR)ì„ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ê²½ìš° (ìˆ˜í•™ ë¬¸ì œ ìœ„ì£¼) ---
        # reward_typeì´ 'pr+vr'ì´ê³ , ë°ì´í„° ì†ŒìŠ¤ê°€ ì§€ì •ëœ ìˆ˜í•™ ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆì„ ë•Œ ì‹¤í–‰
        if self.reward_type == 'pr+vr' and (any(dataset_name in data_source for dataset_name in [
            "numina_cn_k12", "numina_synthetic_math", "numina_olympiads", 
            "numina_synthetic_amc", "numina_aops_forum", "numina_amc_aime",
            "Math-500", "AIME2024", "AMC2023", "DAPO-Math-17k",
            "OlympiadBench", "Minerva", "simplelr_deepscaler"
        ])):

            # ë°ì´í„° ì•„ì´í…œì—ì„œ ì •ë‹µ(GT) í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
            ground_truth = data_item.non_tensor_batch['reward_model']['ground_truth']

            # ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (íŠ¹ìˆ˜ í† í° ì œì™¸)
            solution_str = self.tokenizer.decode(valid_response_ids, skip_special_tokens=True),
            
            # ì™¸ë¶€ ìˆ˜í•™ ì±„ì  ëª¨ë“ˆ(prime_math)ì„ í˜¸ì¶œí•˜ì—¬ ì‹¤ì œ ì •ë‹µ ì—¬ë¶€ íŒë‹¨
            res = prime_math.compute_score(solution_str, ground_truth)
            scoreB = float(res[0]) # ì±„ì  ê²°ê³¼ (ë§ìœ¼ë©´ 1.0, í‹€ë¦¬ë©´ 0.0 ë“±)
            scoreA = 0 # VR ëª¨ë“œì—ì„œëŠ” ê¸°ì € ì ìˆ˜(A)ë¥¼ ë³´í†µ 0ìœ¼ë¡œ ì²˜ë¦¬
            extracted_answer = res[1] # ì±„ì  ê³¼ì •ì—ì„œ íŒŒì‹±ëœ ì •ë‹µ í…ìŠ¤íŠ¸
            
        # --- ë¶„ê¸° 2: ìˆœìˆ˜ í™•ë¥  ê¸°ë°˜ ë³´ìƒ(PR)ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ---
        else:
            # 1. ëª¨ë¸ ë‚´ë¶€ ë¡œê·¸ í™•ë¥ ì„ ì‚¬ìš©í•˜ì—¬ scoreB(r) ê³„ì‚°
            """Score B : reasoning zê°€ ìˆì„ ë•Œ ground truthê°€ ë‚˜ì˜¬ í™•ë¥  ê´€ë ¨ ì ìˆ˜"""
            scoreB = self.compute_scoreB(data_item.batch['old_log_probs_pr'], data_item.batch['ground_truth_mask_pr']) 
            
            # 2. ë¯¸ë¦¬ ê³„ì‚°ë˜ì–´ ì €ì¥ë˜ì–´ ìˆëŠ” ê¸°ì € ì ìˆ˜ scoreA(r')ë¥¼ ê°€ì ¸ì˜´ (ì—†ìœ¼ë©´ 0.0)
           """Score A : reasoning zê°€ ì—†ì„ ë•Œ ground truthê°€ ë‚˜ì˜¬ í™•ë¥  ê´€ë ¨ ì ìˆ˜""" 
     
            scoreA = data_item.non_tensor_batch['reward_model'].get('scoreA', 0.0)

            # 3. ëª¨ë¸ì˜ ì „ì²´ ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ <answer> íƒœê·¸ ì‚¬ì´ì˜ ì •ë‹µì„ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ
            predict_str = self.tokenizer.decode(valid_response_ids) 
            match = re.search(r'<answer>(.*?)</answer>', predict_str, re.DOTALL)
            extracted_answer = match.group(1).strip() if match else ""
            
        return scoreA, scoreB, extracted_answer
``` 

- ê³„ì‚°ëœ scoreë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë³´ìƒì„ í•©ì‚°í•˜ëŠ” ë¶€ë¶„

```python
def __call__(self, data: DataProto):
        """ë°ì´í„°ì…‹ ê°€ìš©ì„±ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ í™•ì¥ë  ë³´ìƒ ê³„ì‚° ë©”ì¸ í•¨ìˆ˜.
        ë°°ì¹˜ ë°ì´í„°ë¥¼ ë°›ì•„ ê° ìƒ˜í”Œì— ëŒ€í•œ ë³´ìƒì„ ê³„ì‚°í•¨.
        """

        # 1. ì´ë¯¸ ì™¸ë¶€ ë³´ìƒ ëª¨ë¸(RM)ë¡œë¶€í„° ê³„ì‚°ëœ ì ìˆ˜ê°€ ë°°ì¹˜ì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì¦‰ì‹œ ë°˜í™˜
        if 'rm_scores' in data.batch.keys():
            return data.batch['rm_scores']

        # 2. ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ê° í•­ëª©ë³„ í…ì„œ ì´ˆê¸°í™” (ì‘ë‹µ ì‹œí€€ìŠ¤ì™€ ë™ì¼í•œ í¬ê¸°)
        reward_tensor = torch.zeros_like(data.batch['responses'], dtype=torch.float32) # ìµœì¢… í†µí•© ë³´ìƒ
        extracted_answer_list = [] # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì •ë‹µ ë¦¬ìŠ¤íŠ¸
        format_reward_tensor = torch.zeros_like(data.batch['responses'], dtype=torch.float32) # í˜•ì‹(íƒœê·¸) ì¤€ìˆ˜ ì ìˆ˜
        scoreA_tensor = torch.zeros_like(data.batch['responses'], dtype=torch.float32) # ê¸°ì € ì ìˆ˜ (Reference Prob without CoT)
        scoreB_tensor = torch.zeros_like(data.batch['responses'], dtype=torch.float32) # ëŒ€ìƒ ì ìˆ˜ (Reference Prob with CoT)

        already_print_data_sources = {} # ë””ë²„ê¹…ìš© ì¶œë ¥ ì¹´ìš´íŠ¸ ê´€ë¦¬ìš© ë”•ì…”ë„ˆë¦¬

        # 3. ë°ì´í„° ë°°ì¹˜ ë‚´ì˜ ê° ê°œë³„ ìƒ˜í”Œ(DataProtoItem)ì„ ìˆœíšŒ
        for i in range(len(data)):
            data_item = data[i]  # ë°°ì¹˜ ë‚´ ië²ˆì§¸ ì•„ì´í…œ

            # --- í”„ë¡¬í”„íŠ¸(ì§ˆë¬¸) ì˜ì—­ ì²˜ë¦¬ ---
            prompt_ids = data_item.batch['prompts']
            prompt_length = prompt_ids.shape[-1] # ê³ ì •ëœ ìµœëŒ€ í”„ë¡¬í”„íŠ¸ ê¸¸ì´

            # íŒ¨ë”©ì„ ì œì™¸í•œ ì‹¤ì œ ìœ íš¨í•œ í”„ë¡¬í”„íŠ¸ í† í°ë§Œ ì¶”ì¶œ (attention_mask í™œìš©)
            valid_prompt_length = data_item.batch['attention_mask'][:prompt_length].sum()
            valid_prompt_ids = prompt_ids[-valid_prompt_length:]

            # --- ì‘ë‹µ(ëª¨ë¸ ë‹µë³€) ì˜ì—­ ì²˜ë¦¬ ---
            response_ids = data_item.batch['responses'] # ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ IDs (ì˜ˆ: ê¸¸ì´ 1024)
            # íŒ¨ë”©ì„ ì œì™¸í•œ ì‹¤ì œ ìœ íš¨í•œ ì‘ë‹µ í† í° ê¸¸ì´ ê³„ì‚° (ì˜ˆ: 329)
            valid_response_length = data_item.batch['attention_mask'][prompt_length:].sum() 
            valid_response_ids = response_ids[:valid_response_length] 

            # --- ë””ì½”ë”© (ID -> Text) ---
            sequences = torch.cat((valid_prompt_ids, valid_response_ids))
            sequences_str = self.tokenizer.decode(sequences) 

            # ì§ˆë¬¸ ì˜ì—­ë§Œ ë””ì½”ë”©
            prompt_str = self.tokenizer.decode(valid_prompt_ids) 
            # ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€ ì˜ì—­ë§Œ ë””ì½”ë”©
            predict_str = self.tokenizer.decode(valid_response_ids) 
            
            # 4. í˜•ì‹ ë³´ìƒ ê³„ì‚°: <think>, <answer> íƒœê·¸ê°€ ê·œê²©ì— ë§ê²Œ í¬í•¨ë˜ì—ˆëŠ”ì§€ ê²€ì‚¬
            format_score = format_reward(predict_str=predict_str, format_mode=self.format_mode)

            # 5. RLPR í•µì‹¬ ìˆ˜ì¹˜ ê³„ì‚°: scoreB(r), scoreA(r') ë° ì¶”ì¶œëœ ì •ë‹µ í…ìŠ¤íŠ¸ íšë“
            scoreA, scoreB, extracted_answer = self.compute_scoreA_scoreB_and_extracted_answer(data_item, valid_response_ids=valid_response_ids)
            
            # 6. ë³´ìƒ ë¸íƒ€ ê³„ì‚° ë° ë³€í˜•: Delta = Shaping(scoreB - scoreA)
            score_delta = scoreB - scoreA
            score_delta = self.shaping_function(score_delta) # ì‹œê·¸ëª¨ì´ë“œ ë“± ì ìš©
            
            # 7. ì´ì‚°í™”(Discrete Binning) ì²˜ë¦¬: í•„ìš”í•œ ê²½ìš° ì—°ì†ì ì¸ ì ìˆ˜ë¥¼ êµ¬ê°„ë³„ë¡œ ë‚˜ëˆ”
            if self.discrete_function_name is not None and self.discrete_function_name != 'identity':
                if self.discrete_function_name.startswith('bin_'):
                    num_bins = int(self.discrete_function_name.split('_')[1])
                    score_delta = self.map_to_bins(score_delta, num_bins)

            # 8. ìµœì¢… ë³´ìƒ í•©ì‚° ë¡œì§
            if self.format_coefficient == -1:
                # ê³„ìˆ˜ê°€ -1ì¸ ê²½ìš°: í˜•ì‹ì´ í‹€ë¦¬ë©´(format_score != 1) ë¬´ì¡°ê±´ -1ì , ë§ìœ¼ë©´ ë¸íƒ€ ì ìˆ˜ ë¶€ì—¬
                score = score_delta if format_score == 1 else -1
            else:
                # ì¼ë°˜ì ì¸ ê²½ìš°: (1 - k) * í™•ë¥ ë³´ìƒ + k * í˜•ì‹ë³´ìƒ ë¹„ìœ¨ë¡œ ê°€ì¤‘ í•©ì‚°
                score = (1 - self.format_coefficient) * (
```